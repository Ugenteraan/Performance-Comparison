# Training with and without Batch Normalization

In the script, there are two models with the same architecture except that one has batch normalization implemented in it and the other without batch normalization. The difference in loss and accuracy during training is tested and the results are plotted as graphs.

## Summary

### Loss Comparison
![]loss.png

### Accuracy Comparison
![]accuracy.png


**License**
-------
**MIT**

